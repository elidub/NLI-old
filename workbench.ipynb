{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nli.data import Vocabulary\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_snli = load_from_disk('data/snli')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "train\n",
      "validation\n"
     ]
    }
   ],
   "source": [
    "for split in dataset_snli:\n",
    "    print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import argparse\n",
    "\n",
    "from nli.data import NLIDataModule, Vocabulary, DataSetPadding\n",
    "from nli.models import AvgWordEmb, UniLSTM, BiLSTM, MaxPoolLSTM, MLP, NLINet\n",
    "from nli.learner import Learner\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AvgWordEmb()\n",
    "b = UniLSTM(2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AvgWordEmb'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [['obvious'], ['horrible'], ['crummy'], ['execrable', '.'], ['shallow', '.'], ['eh', '.'], ['abysmally', 'pathetic'], ['terrible', '.'], ['wishy-washy', '.'], ['spiderman', 'rocks'], ['delightfully', 'rendered'], ['surprisingly', 'insightful'], ['touchM-i', '!'], ['refreshing', '.'], ['fantastic', '!'], ['calculated', 'swill', '.'], ['thoroughly', 'awful', '.'], ['truly', 'terrible', '.'], ['by-the-numbers', 'yarn', '.'], ['amazingly', 'dopey', '.'], ['crikey', 'indeed', '.'], ['so-so', 'entertainment', '.'], ['cinematic', 'poo', '.'], ['extremely', 'bad', '.'], ['woefully', 'pretentious', '.'], ['warmed-over', 'hash', '.'], ['lacks', 'depth', '.'], ['under-rehearsed', 'and', 'lifeless'], ['unwieldy', 'contraption', '.'], ['feeble', 'comedy', '.'], ['disjointed', 'parody', '.'], ['two-bit', 'potboiler', '.'], ['painfully', 'padded', '.'], ['dramatically', 'lackluster', '.'], ['incoherence', 'reigns', '.'], ['mildly', 'amusing', '.'], ['fairly', 'run-of-the-mill', '.'], ['mildly', 'entertaining', '.'], ['insufferably', 'naive', '.'], ['amazingly', 'lame', '.'], ['predictably', 'melodramatic', '.'], ['rashomon-for-dipsticks', 'tale', '.'], ['no', 'surprises', '.'], ['thoroughly', 'enjoyable', '.'], ['family', 'fare', '.'], ['compellingly', 'watchable', '.'], ['often', 'hilarious', '.'], ['psychologically', 'savvy', '.'], ['one-of-a-kind', 'near-masterpiece', '.'], ['highly', 'engaging', '.'], ['beautifully', 'produced', '.'], ['psychologically', 'revealing', '.'], ['visually', 'captivating', '.'], ['morvern', 'rocks', '.'], ['genuinely', 'unnerving', '.'], ['deliciously', 'slow', '.'], ['exciting', 'documentary', '.'], ['a', 'muted', 'freak-out'], ['harmless', 'fun', '.'], ['oddly', 'compelling', '.'], ['delirious', 'fun', '.'], ['quietly', 'engaging', '.'], ['a', 'joyous', 'occasion'], ['everything', 'is', 'off', '.'], ['earnest', 'but', 'heavy-handed', '.'], ['one', 'lousy', 'movie', '.'], ['fluffy', 'and', 'disposible', '.'], ['aan', 'opportunity', 'wasted', '.'], ['storytelling', 'feels', 'slight', '.'], ['a', 'high-minded', 'snoozer', '.'], ['banal', 'and', 'predictable', '.'], ['brisk', 'hack', 'job', '.'], ['punitively', 'affirmational', 'parable', '.'], ['decent', 'but', 'dull', '.'], ['thin', 'period', 'piece', '.'], ['well-meant', 'but', 'unoriginal', '.'], ['odd', 'and', 'weird', '.'], ['draggin', \"'\", 'about', 'dragons'], ['a', 'dreary', 'movie', '.'], ['pompous', 'and', 'garbled', '.'], ['well-meaning', 'but', 'inert', '.'], ['shrewd', 'but', 'pointless', '.'], ['a', 'non-mystery', 'mystery', '.'], ['what', 'an', 'embarrassment', '.'], ['a', 'noble', 'failure', '.'], ['a', 'relative', 'letdown', '.'], ['a', 'puzzling', 'experience', '.'], ['just', 'not', 'campy', 'enough'], ['aggravating', 'and', 'tedious', '.'], ['an', 'awful', 'snooze', '.'], ['just', 'plain', 'silly', '.'], ['a', 'less-than-thrilling', 'thriller', '.'], ['black-and-white', 'and', 'unrealistic', '.'], ['anemic', ',', 'pretentious', '.'], ['i', 'hate', 'this', 'movie'], ['grating', 'and', 'tedious', '.'], ['it', 'bites', 'hard', '.'], ['a', 'pretentious', 'mess', '.'], ['predictably', 'soulless', 'techno-tripe', '.'], ['arty', 'gay', 'film', '.'], ['a', 'half-assed', 'film', '.'], ['bland', 'but', 'harmless', '.'], ['a', 'dreary', 'indulgence', '.'], ['tends', 'to', 'plod', '.'], ['a', 'well-crafted', 'letdown', '.'], ['boring', 'and', 'meandering', '.'], ['less', 'than', 'fresh', '.'], ['a', 'lame', 'comedy', '.'], ['a', 'reality-snubbing', 'hodgepodge', '.'], ['degenerates', 'into', 'hogwash', '.'], ['meandering', 'and', 'confusing', '.'], ['an', 'opportunity', 'missed', '.'], ['inconsequential', 'road-and-buddy', 'pic', '.'], ['a', 'movie', 'to', 'forget'], ['more', 'precious', 'than', 'perspicacious'], ['an', 'intriguing', 'near-miss', '.'], ['bearable', '.', 'barely', '.'], ['staggeringly', 'dreadful', 'romance', '.'], ['well-made', 'but', 'mush-hearted', '.'], ['a', 'real', 'snooze', '.'], ['effective', 'but', 'too-tepid', 'biopic'], ['an', 'imaginative', 'comedy/thriller', '.'], ['an', 'exhilarating', 'experience', '.'], ['troubling', 'and', 'powerful', '.'], ['tailored', 'to', 'entertain', '!'], ['a', 'modest', 'masterpiece', '.'], ['never', 'once', 'predictable', '.'], ['warm', 'and', 'exotic', '.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dictionary: 100%|██████████| 1098734/1098734 [00:02<00:00, 457523.47it/s]\n",
      "Creating word vectors from data/glove.840B.300d.txt:   0%|          | 1000/2196017 [00:00<11:35, 3157.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in sample: 36549\n",
      "Words in wordvec: 779\n",
      "Overlapping words: 779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_snli = load_from_disk('data/snli')\n",
    "samples = dataset_snli['train']['premise'] + dataset_snli['train']['hypothesis']\n",
    "vocab = Vocabulary(samples, path_to_vec = \"data/glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstore/vocab.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     pickle\u001b[39m.\u001b[39mdump(vocab, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "with open('store/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vocab \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(\u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mstore/vocab.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "vocab = pickle.load(open('store/vocab.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open('store/vocab.pkl', 'rb'))\n",
    "\n",
    "hidden_dim = 2048\n",
    "\n",
    "encoder = UniLSTM(hidden_dim)\n",
    "classifier = MLP(hidden_dim*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NLINet(encoder, classifier, vocab)\n",
    "model = Learner(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'lightning_logs/version_0/checkpoints/epoch=2-step=48.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Learner.load_from_checkpoint(ckpt_path, net=net)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_sent = lambda sent: DataSetPadding(None, vocab).prepare_sent(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ids, slens = zip(*[prep_sent(sent) for sent in samples])\n",
    "sent_ids = torch.stack(sent_ids)\n",
    "slens    = torch.tensor(slens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.net.encode(sent_ids, slens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/avg_word_emb/test/checkpoints/epoch=3-step=64.ckpt'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argsversion = 'test'\n",
    "\n",
    "ckpt_path = 'avg_word_emb'\n",
    "\n",
    "versions = os.listdir(os.path.join('logs', ckpt_path))\n",
    "version = versions[0] if len(versions) == 1 else argsversion\n",
    "ckpts = os.listdir(os.path.join('logs', ckpt_path, version, 'checkpoints'))\n",
    "assert len(ckpts) == 1\n",
    "ckpt_path = os.path.join('logs', ckpt_path, version, 'checkpoints', ckpts[0])\n",
    "\n",
    "ckpt_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
