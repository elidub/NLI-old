{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import absolute_import, division, unicode_literals\n",
    "\n",
    "import sys\n",
    "import io\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "# Set PATHs\n",
    "PATH_TO_SENTEVAL = './'\n",
    "PATH_TO_DATA = './SentEval/data'\n",
    "PATH_TO_VEC = './data/glove.840B.300d.txt'\n",
    "\n",
    "# import SentEval\n",
    "sys.path.insert(0, PATH_TO_SENTEVAL)\n",
    "import senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lcur1112/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nli.preprocess import create_dictionary, get_glove as get_wordvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 21:13:52,596 : ***** Transfer task : MR *****\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 117355/2196017 [00:03<01:04, 32307.77it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m se \u001b[39m=\u001b[39m senteval\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39mSE(params_senteval, batcher, prepare)\n\u001b[1;32m     51\u001b[0m transfer_tasks \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mMR\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCR\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMPQA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSUBJ\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSST2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTREC\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     52\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mMRPC\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSICKEntailment\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSTS14\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 53\u001b[0m results \u001b[39m=\u001b[39m se\u001b[39m.\u001b[39;49meval(transfer_tasks)\n\u001b[1;32m     54\u001b[0m \u001b[39mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/SentEval-0.1.0-py3.10.egg/senteval/engine.py:60\u001b[0m, in \u001b[0;36mSE.eval\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(name, \u001b[39mlist\u001b[39m)):\n\u001b[1;32m     59\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mhello\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m {x: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m name}\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults\n\u001b[1;32m     63\u001b[0m tpath \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mtask_path\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/SentEval-0.1.0-py3.10.egg/senteval/engine.py:60\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(name, \u001b[39mlist\u001b[39m)):\n\u001b[1;32m     59\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mhello\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m {x: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m name}\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults\n\u001b[1;32m     63\u001b[0m tpath \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mtask_path\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/SentEval-0.1.0-py3.10.egg/senteval/engine.py:120\u001b[0m, in \u001b[0;36mSE.eval\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation \u001b[39m=\u001b[39m CoordinationInversionEval(tpath \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/probing\u001b[39m\u001b[39m'\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mseed)\n\u001b[1;32m    119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mcurrent_task \u001b[39m=\u001b[39m name\n\u001b[0;32m--> 120\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluation\u001b[39m.\u001b[39;49mdo_prepare(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation\u001b[39m.\u001b[39mrun(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatcher)\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/SentEval-0.1.0-py3.10.egg/senteval/binary.py:29\u001b[0m, in \u001b[0;36mBinaryClassifierEval.do_prepare\u001b[0;34m(self, params, prepare)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_prepare\u001b[39m(\u001b[39mself\u001b[39m, params, prepare):\n\u001b[1;32m     28\u001b[0m     \u001b[39m# prepare is given the whole text\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m prepare(params, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msamples)\n",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(params, samples)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare\u001b[39m(params, samples):\n\u001b[1;32m      3\u001b[0m     _, params\u001b[39m.\u001b[39mword2id \u001b[39m=\u001b[39m create_dictionary(samples)\n\u001b[0;32m----> 4\u001b[0m     params\u001b[39m.\u001b[39mword_vec \u001b[39m=\u001b[39m get_wordvec(params\u001b[39m.\u001b[39;49mword2id, PATH_TO_VEC)\n\u001b[1;32m      5\u001b[0m     params\u001b[39m.\u001b[39mwvec_dim \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/NLI/./nli/preprocess.py:105\u001b[0m, in \u001b[0;36mget_glove\u001b[0;34m(vocab, glove_path)\u001b[0m\n\u001b[1;32m    103\u001b[0m     word, vec \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m vocab:\n\u001b[0;32m--> 105\u001b[0m         wordvec[word] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(\u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(\u001b[39mfloat\u001b[39;49m, vec\u001b[39m.\u001b[39;49msplit())))\n\u001b[1;32m    107\u001b[0m wordvec[\u001b[39m'\u001b[39m\u001b[39m<unk>\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnormal(mean\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, std\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, size\u001b[39m=\u001b[39m(\u001b[39m300\u001b[39m,))\n\u001b[1;32m    108\u001b[0m \u001b[39m# wordvec['<pad>'] = torch.normal(mean=0, std=1, size=(300,))\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SentEval prepare and batcher\n",
    "def prepare(params, samples):\n",
    "    _, params.word2id = create_dictionary(samples)\n",
    "    params.word_vec = get_wordvec(params.word2id, PATH_TO_VEC)\n",
    "    params.wvec_dim = 300\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    print('batcher')\n",
    "    print('params',params)\n",
    "    print('batch', batch)\n",
    "    batch = [sent if sent != [] else ['.'] for sent in batch]\n",
    "    embeddings = []\n",
    "\n",
    "    for sent in batch:\n",
    "        sentvec = []\n",
    "        for word in sent:\n",
    "            if word in params.word_vec:\n",
    "                sentvec.append(params.word_vec[word])\n",
    "        if not sentvec:\n",
    "            vec = np.zeros(params.wvec_dim)\n",
    "            sentvec.append(vec)\n",
    "        sentvec = np.mean(sentvec, 0)\n",
    "        embeddings.append(sentvec)\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def batcher(params, batch):\n",
    "\n",
    "    batch = [sent if sent != [] else ['.'] for sent in batch]\n",
    "    for sent in batch:\n",
    "        for word in sent:\n",
    "            print(word)\n",
    "\n",
    "    assert False\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Set params for SentEval\n",
    "params_senteval = {'task_path': PATH_TO_DATA, 'usepytorch': False, 'kfold': 10}\n",
    "params_senteval['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "                                 'tenacity': 3, 'epoch_size': 2}\n",
    "\n",
    "# Set up logger\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    se = senteval.engine.SE(params_senteval, batcher, prepare)\n",
    "    transfer_tasks = ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC',\n",
    "                      'MRPC', 'SICKEntailment', 'STS14']\n",
    "    results = se.eval(transfer_tasks)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lcur1112/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lcur1112/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nli.preprocess import get_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 20:32:11,082 : open file: /home/lcur1112/NLI/data/snli/dataset_dict.json\n"
     ]
    }
   ],
   "source": [
    "dataset_snli = load_from_disk('data/snli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = dataset_snli['test']['premise'] + dataset_snli['test']['hypothesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word, word2id = create_dictionary(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import argparse\n",
    "\n",
    "from nli.data import NLIDataModule\n",
    "from nli.models import AvgWordEmb, UniLSTM, BiLSTM, MaxPoolLSTM, MLP, NLINet\n",
    "from nli.learner import Learner\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [['obvious'], ['horrible'], ['crummy'], ['execrable', '.'], ['shallow', '.'], ['eh', '.'], ['abysmally', 'pathetic'], ['terrible', '.'], ['wishy-washy', '.'], ['spiderman', 'rocks'], ['delightfully', 'rendered'], ['surprisingly', 'insightful'], ['touchM-i', '!'], ['refreshing', '.'], ['fantastic', '!'], ['calculated', 'swill', '.'], ['thoroughly', 'awful', '.'], ['truly', 'terrible', '.'], ['by-the-numbers', 'yarn', '.'], ['amazingly', 'dopey', '.'], ['crikey', 'indeed', '.'], ['so-so', 'entertainment', '.'], ['cinematic', 'poo', '.'], ['extremely', 'bad', '.'], ['woefully', 'pretentious', '.'], ['warmed-over', 'hash', '.'], ['lacks', 'depth', '.'], ['under-rehearsed', 'and', 'lifeless'], ['unwieldy', 'contraption', '.'], ['feeble', 'comedy', '.'], ['disjointed', 'parody', '.'], ['two-bit', 'potboiler', '.'], ['painfully', 'padded', '.'], ['dramatically', 'lackluster', '.'], ['incoherence', 'reigns', '.'], ['mildly', 'amusing', '.'], ['fairly', 'run-of-the-mill', '.'], ['mildly', 'entertaining', '.'], ['insufferably', 'naive', '.'], ['amazingly', 'lame', '.'], ['predictably', 'melodramatic', '.'], ['rashomon-for-dipsticks', 'tale', '.'], ['no', 'surprises', '.'], ['thoroughly', 'enjoyable', '.'], ['family', 'fare', '.'], ['compellingly', 'watchable', '.'], ['often', 'hilarious', '.'], ['psychologically', 'savvy', '.'], ['one-of-a-kind', 'near-masterpiece', '.'], ['highly', 'engaging', '.'], ['beautifully', 'produced', '.'], ['psychologically', 'revealing', '.'], ['visually', 'captivating', '.'], ['morvern', 'rocks', '.'], ['genuinely', 'unnerving', '.'], ['deliciously', 'slow', '.'], ['exciting', 'documentary', '.'], ['a', 'muted', 'freak-out'], ['harmless', 'fun', '.'], ['oddly', 'compelling', '.'], ['delirious', 'fun', '.'], ['quietly', 'engaging', '.'], ['a', 'joyous', 'occasion'], ['everything', 'is', 'off', '.'], ['earnest', 'but', 'heavy-handed', '.'], ['one', 'lousy', 'movie', '.'], ['fluffy', 'and', 'disposible', '.'], ['aan', 'opportunity', 'wasted', '.'], ['storytelling', 'feels', 'slight', '.'], ['a', 'high-minded', 'snoozer', '.'], ['banal', 'and', 'predictable', '.'], ['brisk', 'hack', 'job', '.'], ['punitively', 'affirmational', 'parable', '.'], ['decent', 'but', 'dull', '.'], ['thin', 'period', 'piece', '.'], ['well-meant', 'but', 'unoriginal', '.'], ['odd', 'and', 'weird', '.'], ['draggin', \"'\", 'about', 'dragons'], ['a', 'dreary', 'movie', '.'], ['pompous', 'and', 'garbled', '.'], ['well-meaning', 'but', 'inert', '.'], ['shrewd', 'but', 'pointless', '.'], ['a', 'non-mystery', 'mystery', '.'], ['what', 'an', 'embarrassment', '.'], ['a', 'noble', 'failure', '.'], ['a', 'relative', 'letdown', '.'], ['a', 'puzzling', 'experience', '.'], ['just', 'not', 'campy', 'enough'], ['aggravating', 'and', 'tedious', '.'], ['an', 'awful', 'snooze', '.'], ['just', 'plain', 'silly', '.'], ['a', 'less-than-thrilling', 'thriller', '.'], ['black-and-white', 'and', 'unrealistic', '.'], ['anemic', ',', 'pretentious', '.'], ['i', 'hate', 'this', 'movie'], ['grating', 'and', 'tedious', '.'], ['it', 'bites', 'hard', '.'], ['a', 'pretentious', 'mess', '.'], ['predictably', 'soulless', 'techno-tripe', '.'], ['arty', 'gay', 'film', '.'], ['a', 'half-assed', 'film', '.'], ['bland', 'but', 'harmless', '.'], ['a', 'dreary', 'indulgence', '.'], ['tends', 'to', 'plod', '.'], ['a', 'well-crafted', 'letdown', '.'], ['boring', 'and', 'meandering', '.'], ['less', 'than', 'fresh', '.'], ['a', 'lame', 'comedy', '.'], ['a', 'reality-snubbing', 'hodgepodge', '.'], ['degenerates', 'into', 'hogwash', '.'], ['meandering', 'and', 'confusing', '.'], ['an', 'opportunity', 'missed', '.'], ['inconsequential', 'road-and-buddy', 'pic', '.'], ['a', 'movie', 'to', 'forget'], ['more', 'precious', 'than', 'perspicacious'], ['an', 'intriguing', 'near-miss', '.'], ['bearable', '.', 'barely', '.'], ['staggeringly', 'dreadful', 'romance', '.'], ['well-made', 'but', 'mush-hearted', '.'], ['a', 'real', 'snooze', '.'], ['effective', 'but', 'too-tepid', 'biopic'], ['an', 'imaginative', 'comedy/thriller', '.'], ['an', 'exhilarating', 'experience', '.'], ['troubling', 'and', 'powerful', '.'], ['tailored', 'to', 'entertain', '!'], ['a', 'modest', 'masterpiece', '.'], ['never', 'once', 'predictable', '.'], ['warm', 'and', 'exotic', '.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = AvgWordEmb()\n",
    "classifier = MLP(300*4)\n",
    "\n",
    "net = NLINet(encoder, classifier)\n",
    "model = Learner(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lightning_logs/version_11564821/checkpoints/epoch=8-step=144.ckpt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = 'lightning_logs/version_11564439'\n",
    "ckpt_path = 'lightning_logs/version_11564821'\n",
    "ckpts = os.listdir(os.path.join(ckpt_path, 'checkpoints'))\n",
    "assert len(ckpts) == 1\n",
    "ckpt_path = os.path.join(ckpt_path, 'checkpoints', ckpts[0])\n",
    "ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(\n",
       "  (net): NLINet(\n",
       "    (encoder): AvgWordEmb()\n",
       "    (classifier): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1200, out_features=512, bias=True)\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (2): Linear(in_features=512, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Learner.load_from_checkpoint(ckpt_path, net=net)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [Learner] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(samples)\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/pytorch_lightning/core/module.py:644\u001b[0m, in \u001b[0;36mLightningModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    634\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[39m    Same as :meth:`torch.nn.Module.forward()`.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39m        Your model's output\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/torch/nn/modules/module.py:244\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_unimplemented\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[39m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] is missing the required \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mforward\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m function\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [Learner] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "model.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lightning_logs/version_11564821/checkpoints/epoch=8-step=144.ckpt'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Learner:\n\tMissing key(s) in state_dict: \"net.encoder.lstm.weight_ih_l0\", \"net.encoder.lstm.weight_hh_l0\", \"net.encoder.lstm.bias_ih_l0\", \"net.encoder.lstm.bias_hh_l0\", \"net.encoder.lstm.weight_ih_l0_reverse\", \"net.encoder.lstm.weight_hh_l0_reverse\", \"net.encoder.lstm.bias_ih_l0_reverse\", \"net.encoder.lstm.bias_hh_l0_reverse\". \n\tsize mismatch for net.classifier.mlp.0.weight: copying a param with shape torch.Size([512, 1200]) from checkpoint, the shape in current model is torch.Size([512, 16384]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(ckpt_path, map_location\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m storage, loc: storage)\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(checkpoint[\u001b[39m'\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/torch/nn/modules/module.py:1667\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1662\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1663\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1664\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1666\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1667\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1669\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Learner:\n\tMissing key(s) in state_dict: \"net.encoder.lstm.weight_ih_l0\", \"net.encoder.lstm.weight_hh_l0\", \"net.encoder.lstm.bias_ih_l0\", \"net.encoder.lstm.bias_hh_l0\", \"net.encoder.lstm.weight_ih_l0_reverse\", \"net.encoder.lstm.weight_hh_l0_reverse\", \"net.encoder.lstm.bias_ih_l0_reverse\", \"net.encoder.lstm.bias_hh_l0_reverse\". \n\tsize mismatch for net.classifier.mlp.0.weight: copying a param with shape torch.Size([512, 1200]) from checkpoint, the shape in current model is torch.Size([512, 16384])."
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nli.learner.Learner"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Learner.__init__() missing 1 required positional argument: 'net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mload_from_checkpoint(ckpt_path)\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:137\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     65\u001b[0m ):\n\u001b[1;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_from_checkpoint(\n\u001b[1;32m    138\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m    139\u001b[0m         checkpoint_path,\n\u001b[1;32m    140\u001b[0m         map_location,\n\u001b[1;32m    141\u001b[0m         hparams_file,\n\u001b[1;32m    142\u001b[0m         strict,\n\u001b[1;32m    143\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    144\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:205\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, pl\u001b[39m.\u001b[39mLightningDataModule):\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_state(\u001b[39mcls\u001b[39m, checkpoint, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 205\u001b[0m \u001b[39mreturn\u001b[39;00m _load_state(\u001b[39mcls\u001b[39;49m, checkpoint, strict\u001b[39m=\u001b[39;49mstrict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:250\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m cls_spec\u001b[39m.\u001b[39mvarkw:\n\u001b[1;32m    247\u001b[0m     \u001b[39m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     _cls_kwargs \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m _cls_kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m cls_init_args_name}\n\u001b[0;32m--> 250\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_cls_kwargs)\n\u001b[1;32m    252\u001b[0m \u001b[39m# give model a chance to load something\u001b[39;00m\n\u001b[1;32m    253\u001b[0m obj\u001b[39m.\u001b[39mon_load_checkpoint(checkpoint)\n",
      "\u001b[0;31mTypeError\u001b[0m: Learner.__init__() missing 1 required positional argument: 'net'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
