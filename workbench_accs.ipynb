{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "from __future__ import absolute_import, division, unicode_literals\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import logging\n",
    "import sklearn\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, 'nli/')\n",
    "from setup import load_model, prep_sent, find_checkpoint\n",
    "from data import NLIDataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferResults:\n",
    "    def __init__(self, args, tasks_with_acc_given = None):\n",
    "        \n",
    "        # Read the results\n",
    "        _, version_path = find_checkpoint(args.ckpt_path, args.version)\n",
    "        # with open(os.path.join(version_path, 'results.txt'), 'r') as f:\n",
    "        with open(os.path.join('results.txt'), 'r') as f:\n",
    "            results = json.load(f)\n",
    "        self.results = results\n",
    "\n",
    "        # Assert that the tasks with acc are the same as the ones given\n",
    "        self.tasks_with_acc = self.get_tasks_with_acc(tasks_with_acc_given)\n",
    "\n",
    "    def get_tasks_with_acc(self, tasks_with_acc_given):\n",
    "        task_with_acc = {task for task in self.results if 'acc' in self.results[task]}\n",
    "        if tasks_with_acc_given is not None:\n",
    "            assert task_with_acc == tasks_with_acc_given, f'{task_with_acc} != {tasks_with_acc_given}'\n",
    "        return task_with_acc\n",
    "\n",
    "    def get_transfer_accs(self):\n",
    "        dev_accs = {}\n",
    "        num_dev_samples = {}\n",
    "        for task, task_data in self.results.items():\n",
    "            if task not in self.tasks_with_acc:\n",
    "                continue\n",
    "            dev_accs[task] = task_data['devacc']\n",
    "            num_dev_samples[task] = task_data['ndev']\n",
    "\n",
    "        # Calculate macro accuracy\n",
    "        macro_acc = sum(dev_accs.values()) / len(dev_accs)\n",
    "\n",
    "        # Calculate micro accuracy\n",
    "        total_dev_samples = sum(num_dev_samples.values())\n",
    "        micro_acc = sum(dev_accs[task] * num_dev_samples[task] / total_dev_samples for task in dev_accs)\n",
    "\n",
    "        return {'micro': micro_acc, 'macro': macro_acc}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, model_type = 'avg_word_emb', ckpt_path = None, version = 'version_0', path_to_vocab = 'store/vocab.pkl', num_workers = 3):\n",
    "        self.model_type = model_type\n",
    "        self.ckpt_path = model_type if ckpt_path is None else ckpt_path\n",
    "        self.version = version\n",
    "        self.path_to_vocab = path_to_vocab\n",
    "        self.num_workers = num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a00b0c167d94b86a2b19a09745aef6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0c2f9559a34eb69c8606135897801a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NLIResults:\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.model, vocab = load_model(args.model_type, args.path_to_vocab, args.ckpt_path, args.version)\n",
    "        self.datamodule = NLIDataModule(vocab=vocab, batch_size=64, num_workers=args.num_workers)\n",
    "        self.trainer = pl.Trainer(\n",
    "            logger = False,\n",
    "            accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        )\n",
    "\n",
    "    def test(self):\n",
    "        test_acc = self.trainer.test(self.model, datamodule=self.datamodule, verbose = False)[0]['test_acc']\n",
    "        return test_acc\n",
    "    \n",
    "    def validate(self):\n",
    "        val_acc = self.trainer.validate(self.model, datamodule=self.datamodule, verbose = False)[0]['val_acc']\n",
    "        return val_acc\n",
    "    \n",
    "    def get_nli_accs(self):\n",
    "        return {'test': self.test()*100., 'val': self.validate()*100.}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['avg_word_emb', 'uni_lstm', 'bi_lstm', 'max_pool_lstm']\n",
    "results = {model_type : {} for model_type in model_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_word_emb': {'nli': 'test'}, 'uni_lstm': {'nli': 'test'}, 'bi_lstm': {'nli': 'test'}, 'max_pool_lstm': {'nli': 'test'}}\n"
     ]
    }
   ],
   "source": [
    "for model_type, accs in results.items():\n",
    "    accs['nli'] = 'test'\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[39m=\u001b[39m Args(model_type \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mavg_word_emb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m nli_results \u001b[39m=\u001b[39m NLIResults(args)\n\u001b[1;32m      4\u001b[0m transfer_avg_word_emb \u001b[39m=\u001b[39m TransferResults(\u001b[39m'\u001b[39m\u001b[39mavg_word_emb\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mversion_0\u001b[39m\u001b[39m'\u001b[39m, {\u001b[39m'\u001b[39m\u001b[39mMR\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCR\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m      5\u001b[0m transfer_accs \u001b[39m=\u001b[39m transfer_avg_word_emb\u001b[39m.\u001b[39mget_transfer_accs()\n",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m, in \u001b[0;36mNLIResults.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, vocab \u001b[39m=\u001b[39m load_model(args\u001b[39m.\u001b[39mmodel_type, args\u001b[39m.\u001b[39mpath_to_vocab, args\u001b[39m.\u001b[39mckpt_path, args\u001b[39m.\u001b[39mversion)\n\u001b[1;32m      5\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatamodule \u001b[39m=\u001b[39m NLIDataModule(vocab\u001b[39m=\u001b[39mvocab, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, num_workers\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mnum_workers)\n\u001b[0;32m----> 6\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mTrainer(\n\u001b[1;32m      7\u001b[0m     logger \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m     accelerator\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available() \u001b[39melse\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py:345\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m    344\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:534\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_grad_norm: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(track_grad_norm)\n\u001b[1;32m    533\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m detect_anomaly\n\u001b[0;32m--> 534\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_on_init()\n\u001b[1;32m    536\u001b[0m \u001b[39m# configure tuner\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtuner\u001b[39m.\u001b[39mon_trainer_init(auto_lr_find, auto_scale_batch_size)\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:619\u001b[0m, in \u001b[0;36mTrainer._setup_on_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_setup_on_init\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_device_info()\n\u001b[1;32m    621\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_stop \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    622\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m TrainerState()\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._log_device_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m rank_zero_info(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHPU available: \u001b[39m\u001b[39m{\u001b[39;00m_HPU_AVAILABLE\u001b[39m}\u001b[39;00m\u001b[39m, using: \u001b[39m\u001b[39m{\u001b[39;00mnum_hpus\u001b[39m}\u001b[39;00m\u001b[39m HPUs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1762\u001b[0m \u001b[39m# TODO: Integrate MPS Accelerator here, once gpu maps to both\u001b[39;00m\n\u001b[0;32m-> 1763\u001b[0m \u001b[39mif\u001b[39;00m CUDAAccelerator\u001b[39m.\u001b[39;49mis_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, CUDAAccelerator):\n\u001b[1;32m   1764\u001b[0m     rank_zero_warn(\n\u001b[1;32m   1765\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGPU available but not used. Set `accelerator` and `devices` using\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1766\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m `Trainer(accelerator=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, devices=\u001b[39m\u001b[39m{\u001b[39;00mCUDAAccelerator\u001b[39m.\u001b[39mauto_device_count()\u001b[39m}\u001b[39;00m\u001b[39m)`.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1767\u001b[0m         category\u001b[39m=\u001b[39mPossibleUserWarning,\n\u001b[1;32m   1768\u001b[0m     )\n\u001b[1;32m   1770\u001b[0m \u001b[39mif\u001b[39;00m _TPU_AVAILABLE \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, TPUAccelerator):\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/pytorch_lightning/accelerators/cuda.py:91\u001b[0m, in \u001b[0;36mCUDAAccelerator.is_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_available\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mreturn\u001b[39;00m device_parser\u001b[39m.\u001b[39;49mnum_cuda_devices() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/site-packages/pytorch_lightning/utilities/device_parser.py:348\u001b[0m, in \u001b[0;36mnum_cuda_devices\u001b[0;34m()\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    347\u001b[0m \u001b[39mwith\u001b[39;00m multiprocessing\u001b[39m.\u001b[39mget_context(\u001b[39m\"\u001b[39m\u001b[39mfork\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mPool(\u001b[39m1\u001b[39m) \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m--> 348\u001b[0m     \u001b[39mreturn\u001b[39;00m pool\u001b[39m.\u001b[39;49mapply(torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mdevice_count)\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/multiprocessing/pool.py:360\u001b[0m, in \u001b[0;36mPool.apply\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, func, args\u001b[39m=\u001b[39m(), kwds\u001b[39m=\u001b[39m{}):\n\u001b[1;32m    356\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[39m    Equivalent of `func(*args, **kwds)`.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39m    Pool must be running.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_async(func, args, kwds)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.conda/envs/nli/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = Args(model_type = 'avg_word_emb')\n",
    "\n",
    "nli_results = NLIResults(args)\n",
    "transfer_avg_word_emb = TransferResults(args, {'MR', 'CR'})\n",
    "\n",
    "transfer_accs = transfer_avg_word_emb.get_transfer_accs()\n",
    "nli_accs = nli_results.get_nli_accs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': 0.6547231078147888, 'val': 0.6559642553329468}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test': 0.6547231078147888,\n",
       " 'val': 0.6559642553329468,\n",
       " 'micro': 51.42129389762415,\n",
       " 'macro': 52.435}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs2 = {**accs, **nli_accs}\n",
    "print(accs2)\n",
    "\n",
    "accs2 = {**accs2, **transfer_accs}\n",
    "accs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'micro': 51.42129389762415, 'macro': 52.435}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate two dictionaries python nli_accs and tranfer_accs\n",
    "accs = {**nli_accs, **transfer_accs}\n",
    "\n",
    "# round all acs to 2 decimal places\n",
    "accs = {k: round(v, 1) for k, v in accs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 0.7, 'val': 0.7, 'micro': 51.4, 'macro': 52.4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, version_path = find_checkpoint(args.ckpt_path, args.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/avg_word_emb/version_0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
